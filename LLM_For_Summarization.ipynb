{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n"
      ],
      "metadata": {
        "id": "jv2Eu_1gx9Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import html\n",
        "import unicodedata\n",
        "\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "yaW-CXJ-x_gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Input Embeddings"
      ],
      "metadata": {
        "id": "JftuuklsykRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, dim_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        # Dimension of Our Model\n",
        "        self.dim_model = dim_model\n",
        "        # Vocabulary Size of Our Model\n",
        "        self.vocab_size = vocab_size\n",
        "        # Creating Embedding For Each Vocab Word\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multipied to Normalize as random values my be too small\n",
        "        return self.embedding(x) * math.sqrt(self.dim_model)"
      ],
      "metadata": {
        "id": "2QT55JjRyYYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "Eob3YvjK4CKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        # Dimension of Our Model\n",
        "        self.dim_model = dim_model\n",
        "        # Sequence Length of Our Model\n",
        "        self.seq_len = seq_len\n",
        "        # Dropout to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # using unsqueeze to add extra dimension to match shape for multiplication: [seq_len,1]\n",
        "        token_positions = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n",
        "        # creating division term for positional encoding formula for pairs (sin=2i,cos=2i+1)\n",
        "        divider = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model))\n",
        "        # Creating matrix intialized with zeros\n",
        "        positional_encoding = torch.zeros(seq_len, dim_model)\n",
        "        # Applying sine to even indices (0,2,4,...) in positional encoding\n",
        "        positional_encoding[:, 0::2] = torch.sin(token_positions * divider)\n",
        "        # Applying cosine to odd indices (1,3,5,...) in positional encoding\n",
        "        positional_encoding[:, 1::2] = torch.cos(token_positions * divider)\n",
        "\n",
        "        # Adding an extra dimension at the beginning of positional encoding matrix for batch handling\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "\n",
        "        # Register the positional encoding as a buffer so it moves with the model- either cpu or gpu\n",
        "        self.register_buffer('positional_encoding', positional_encoding)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # extracting the sequence length of current input\n",
        "        seq_len = x.shape[1]\n",
        "        # getting out positional encodings up to the required sequence length\n",
        "        pos_encoding_for_seq = self.positional_encoding[:, :seq_len, :].to(x.device)\n",
        "        # adding positional encoding to the input x\n",
        "        x = x + pos_encoding_for_seq\n",
        "        # Applying Dropout\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s_bPqsmh4Fhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "g2dcqXwfCBMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Defining epsilon as 0.000001 to avoid division by zero\n",
        "        self.epsilon = 10**-6\n",
        "        # defining trainable parameter alpha\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        # defining trainable parameter bias\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # computing mean and std dev accross the embdedding dim\n",
        "        mean = x.mean(dim = -1, keepdim = True)\n",
        "        std = x.std(dim = -1, keepdim = True)\n",
        "        normalized_input = self.alpha * (x - mean)/(std + self.epsilon) + self.bias\n",
        "        return normalized_input"
      ],
      "metadata": {
        "id": "B71T5_awCFbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward"
      ],
      "metadata": {
        "id": "q7Oc_cxfGamB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_model: int, hidden_dim: int, dropout: float):\n",
        "        super().__init__()\n",
        "        # projecting to a larger dimension/expansion layer\n",
        "        self.input_projection = nn.Linear(dim_model, hidden_dim)\n",
        "        # applying dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # transforming back to original\n",
        "        self.output_projection = nn.Linear(hidden_dim, dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input=torch.relu(self.input_projection(x))\n",
        "        output=self.output_projection(self.dropout(input))\n",
        "        return output"
      ],
      "metadata": {
        "id": "DT4Tcz9OF_hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multihead Attention"
      ],
      "metadata": {
        "id": "UFdGiXT3LQVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_model: int, num_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # checking model dimension is divisble by number of heads or not\n",
        "        assert dim_model % num_heads == 0, 'Model Dimensions are not divisible by number of heads'\n",
        "\n",
        "       # dimension of each heads key,query and value matrix\n",
        "        self.dim_k = dim_model // num_heads\n",
        "\n",
        "        # weight matrices for query,key,value and output\n",
        "        self.w_q = nn.Linear(dim_model, dim_model)\n",
        "        self.w_k = nn.Linear(dim_model, dim_model)\n",
        "        self.w_v = nn.Linear(dim_model, dim_model)\n",
        "        self.w_o = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # performing linear tranformation/multiplying q with w_q to get query and others are same\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "\n",
        "        # split matrices for different heads/splitting the dimensions\n",
        "        # shape[0]=batch size,shape[1]=seq_len and swapping 2nd and 3rd dim by transpose\n",
        "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.dim_k).transpose(1,2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.dim_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.dim_k).transpose(1,2)\n",
        "\n",
        "        # Attention calculation\n",
        "        dim_k = query.shape[-1]\n",
        "        # Qxk^t / sqrt(dim_k)\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(dim_k)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            # Handle different mask types:\n",
        "            # 1. Padding mask: [batch_size, 1, seq_len] -> needs reshape\n",
        "            # 2. Attention mask: [batch_size, 1, seq_len, seq_len] -> already correct shape\n",
        "            if mask.dim() == 3:  # For padding mask\n",
        "                batch_size, _, seq_len = mask.shape\n",
        "                # Reshape to [batch_size, 1, 1, seq_len] for broadcasting\n",
        "                mask = mask.view(batch_size, 1, 1, seq_len)\n",
        "\n",
        "            # Apply mask\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        if self.dropout is not None:\n",
        "            # apply dropout\n",
        "            attention_scores = self.dropout(attention_scores)\n",
        "\n",
        "        # applying softmax\n",
        "        attention_scores = attention_scores.softmax(dim = -1)\n",
        "        # multiplying with value matrix\n",
        "        x = torch.matmul(attention_scores, value)\n",
        "        self.attention_scores = attention_scores\n",
        "\n",
        "        # converting back to same shape to get the head concatenated\n",
        "        h = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.dim_k)\n",
        "        # multiplying head matrix with the weight matrix\n",
        "        result=self.w_o(h)\n",
        "        return result\n",
        "\n"
      ],
      "metadata": {
        "id": "2Va9HeyCLfr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Encoder Block"
      ],
      "metadata": {
        "id": "cM2UWGdmUCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.norm2 = LayerNormalization()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Applying Multihead self attention with residual connection\n",
        "        attn_output = self.self_attention(self.norm1(x), self.norm1(x), self.norm1(x), mask)      # Passing x as query,key and value along mask\n",
        "        x = x + self.dropout(attn_output)\n",
        "\n",
        "        # Applying Feed forward Network with residual connection\n",
        "        ff_output = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LQSqc09CUIJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Encoder"
      ],
      "metadata": {
        "id": "NV5cmda9YDgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers: int,self_attention: MultiHeadAttention,feed_forward: FeedForward,dropout: float):\n",
        "        super().__init__()\n",
        "        # Creating N identical encoder blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(\n",
        "                self_attention=self_attention,\n",
        "                feed_forward=feed_forward,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Passing input through each encoder block in sequence\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Final layer normalization\n",
        "        normalized_output = self.norm(x)\n",
        "        return normalized_output"
      ],
      "metadata": {
        "id": "NlXmYOZzYu8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Decoder Block"
      ],
      "metadata": {
        "id": "Sk61Bu9HZ6nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,self_attention_block: MultiHeadAttention,cross_attention_block: MultiHeadAttention,feed_forward_block: FeedForward,dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.norm2 = LayerNormalization()\n",
        "        self.norm3 = LayerNormalization()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # Self attention with residual connection\n",
        "        self_attn = self.self_attention_block(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask)\n",
        "        x = x + self.dropout(self_attn)\n",
        "\n",
        "        # Cross attention with residual connection\n",
        "        cross_attn = self.cross_attention_block(self.norm2(x), encoder_output, encoder_output, src_mask)\n",
        "        x = x + self.dropout(cross_attn)\n",
        "\n",
        "        # Feed forward with residual connection\n",
        "        ff_output = self.feed_forward_block(self.norm3(x))\n",
        "        x = x + self.dropout(ff_output)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_9YMec6YZ-TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Decoder"
      ],
      "metadata": {
        "id": "8oK98kBXv6gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,num_layers: int,self_attention_block: MultiHeadAttention,cross_attention_block: MultiHeadAttention,feed_forward_block: FeedForward,dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create N identical decoder blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                self_attention_block=self_attention_block,\n",
        "                cross_attention_block=cross_attention_block,\n",
        "                feed_forward_block=feed_forward_block,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # Pass through each decoder block in sequence\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        # Final layer normalization\n",
        "        normalized_output = self.norm(x)\n",
        "\n",
        "        return normalized_output\n"
      ],
      "metadata": {
        "id": "pimF7ZFJv8cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Layer with Softmax"
      ],
      "metadata": {
        "id": "E4ypFOeV-AqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linearprojectionlayer(nn.Module):\n",
        "    def __init__(self, dim_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim = -1)\n"
      ],
      "metadata": {
        "id": "rvWsPTXd982m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Transformer Class"
      ],
      "metadata": {
        "id": "n9tO02XJ_Wok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            encoder_vocab_size: int,\n",
        "            decoder_vocab_size: int,\n",
        "            dim_model: int,\n",
        "            num_heads: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            hidden_dim: int,\n",
        "            dropout: float,\n",
        "            max_seq_length: int\n",
        "        ):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "        self.encoder_vocab_size = encoder_vocab_size\n",
        "        self.decoder_vocab_size = decoder_vocab_size\n",
        "        # Creating the embedding layers\n",
        "        self.encoder_embedding = InputEmbeddings(dim_model, encoder_vocab_size)\n",
        "        self.decoder_embedding = InputEmbeddings(dim_model, decoder_vocab_size)\n",
        "\n",
        "        # Creating the positional encoding layers\n",
        "        self.encoder_position = PositionalEncoding(dim_model, max_seq_length, dropout)\n",
        "        self.decoder_position = PositionalEncoding(dim_model, max_seq_length, dropout)\n",
        "\n",
        "        # Creating attention and feed-forward instances for encoder\n",
        "        encoder_self_attention = MultiHeadAttention(dim_model, num_heads, dropout)\n",
        "        encoder_feed_forward = FeedForward(dim_model, hidden_dim, dropout)\n",
        "\n",
        "        # Creating attention and feed-forward instances for decoder\n",
        "        decoder_self_attention = MultiHeadAttention(dim_model, num_heads, dropout)\n",
        "        decoder_cross_attention = MultiHeadAttention(dim_model, num_heads, dropout)\n",
        "        decoder_feed_forward = FeedForward(dim_model, hidden_dim, dropout)\n",
        "\n",
        "        # Creating the encoder and decoder\n",
        "        self.encoder = Encoder(num_encoder_layers,encoder_self_attention,encoder_feed_forward,dropout)\n",
        "\n",
        "        self.decoder = Decoder(num_decoder_layers,decoder_self_attention,decoder_cross_attention,decoder_feed_forward,dropout)\n",
        "\n",
        "        # Creating the final linear projection layer\n",
        "        self.linear = Linearprojectionlayer(dim_model, decoder_vocab_size)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.encoder_embedding(src)\n",
        "        src = self.encoder_position(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "        tgt = self.decoder_embedding(tgt)\n",
        "        tgt = self.decoder_position(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "        return self.linear(decoder_output)\n",
        "\n",
        "\n",
        "    def generate(self, src, src_mask, max_length=100, temperature=1.0):\n",
        "        self.eval()\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "        # Start with START token (2)\n",
        "        tgt = torch.tensor([[2]], device=src.device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Creating causal mask for target\n",
        "            tgt_mask = torch.zeros((1, 1, tgt.size(1), tgt.size(1)), device=src.device)\n",
        "            tgt_mask = torch.triu(tgt_mask.fill_(float('-inf')), diagonal=1)\n",
        "\n",
        "            decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "            logits = self.linear(decoder_output[:, -1:])\n",
        "\n",
        "            # Apply temperature and sample\n",
        "            probs = (logits / temperature).softmax(dim=-1)\n",
        "            next_token = torch.multinomial(probs.squeeze(1), 1)\n",
        "\n",
        "            tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "            # Check for END token 3\n",
        "            if next_token.item() == 3:\n",
        "                break\n",
        "\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "kVa8fQac_WU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Transformer"
      ],
      "metadata": {
        "id": "YEGKIaVxqfh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model hyperparameters\n",
        "# BATCH_SIZE = 2\n",
        "# SEQ_LENGTH = 10\n",
        "# SRC_VOCAB_SIZE = 5000\n",
        "# TGT_VOCAB_SIZE = 5000\n",
        "# DIM_MODEL = 512\n",
        "# NUM_HEADS = 8\n",
        "# NUM_ENCODER_LAYERS = 3\n",
        "# NUM_DECODER_LAYERS = 3\n",
        "# HIDDEN_DIM = 2048\n",
        "# DROPOUT = 0.1\n",
        "# MAX_SEQ_LENGTH = 100\n",
        "\n",
        "# # Create model\n",
        "# model = Transformer(\n",
        "#     encoder_vocab_size=SRC_VOCAB_SIZE,\n",
        "#     decoder_vocab_size=TGT_VOCAB_SIZE,\n",
        "#     dim_model=DIM_MODEL,\n",
        "#     num_heads=NUM_HEADS,\n",
        "#     num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "#     num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "#     hidden_dim=HIDDEN_DIM,\n",
        "#     dropout=DROPOUT,\n",
        "#     max_seq_length=MAX_SEQ_LENGTH\n",
        "# )\n",
        "\n",
        "# # Create sample input data\n",
        "# src = torch.randint(1, SRC_VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))  # Random source sequences\n",
        "# tgt = torch.randint(1, TGT_VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))  # Random target sequences\n",
        "\n",
        "# # Create masks with correct shapes\n",
        "# src_mask = torch.ones(BATCH_SIZE, 1, 1, SEQ_LENGTH)  # Shape: [batch_size, 1, 1, seq_len]\n",
        "\n",
        "# # Create causal mask for decoder\n",
        "# tgt_mask = torch.triu(torch.ones((SEQ_LENGTH, SEQ_LENGTH)) * float('-inf'), diagonal=1)\n",
        "# tgt_mask = tgt_mask.expand(BATCH_SIZE, 1, SEQ_LENGTH, SEQ_LENGTH)\n",
        "\n",
        "\n",
        "# # Forward pass\n",
        "# print(\"Input shapes:\")\n",
        "# print(f\"Source: {src.shape}\")\n",
        "# print(f\"Target: {tgt.shape}\")\n",
        "# print(f\"Source mask: {src_mask.shape}\")\n",
        "# print(f\"Target mask: {tgt_mask.shape}\")\n",
        "\n",
        "# output = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "# print(\"\\nOutput shape:\", output.shape)\n",
        "# print(\"\\nExpected output shape:\", (BATCH_SIZE, SEQ_LENGTH, TGT_VOCAB_SIZE))"
      ],
      "metadata": {
        "id": "Hh9Zr31EB19W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "hhpxOg2Dqjvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, max_vocab_size: int = 50000):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        # Add special token for speaker separator\n",
        "        self.word2idx = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<START>': 2,\n",
        "            '<END>': 3,\n",
        "            '<SPEAKER>': 4,  # New special token for speakers\n",
        "            ':': 5  # Preserve colon as special token\n",
        "        }\n",
        "        self.idx2word = {\n",
        "            0: '<PAD>',\n",
        "            1: '<UNK>',\n",
        "            2: '<START>',\n",
        "            3: '<END>',\n",
        "            4: '<SPEAKER>',\n",
        "            5: ':'\n",
        "        }\n",
        "        self.vocab = Counter()\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        tokens = []\n",
        "        segments = text.split()\n",
        "\n",
        "        for segment in segments:\n",
        "            if ':' in segment:\n",
        "                speaker, rest = segment.split(':', 1)\n",
        "                # Add speaker tokens in sequence\n",
        "                tokens.append('<SPEAKER>')\n",
        "                tokens.extend(re.findall(r'\\b\\w+\\b', speaker.lower()))\n",
        "                tokens.append(':')\n",
        "                # Add message tokens\n",
        "                if rest:\n",
        "                    tokens.extend(re.findall(r'\\b\\w+\\b', rest.lower()))\n",
        "            else:\n",
        "                tokens.extend(re.findall(r'\\b\\w+\\b', segment.lower()))\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def build_vocab(self, texts: List[str]):\n",
        "        # Count words in all texts\n",
        "        for text in texts:\n",
        "            # converting to tokens\n",
        "            tokens = self.tokenize(text)\n",
        "            self.vocab.update(tokens)\n",
        "\n",
        "        # Get most common words\n",
        "        vocab_size = self.max_vocab_size - len(self.word2idx)\n",
        "        # getting most common words first and then adding them to vocab\n",
        "        most_common = self.vocab.most_common(vocab_size)\n",
        "\n",
        "        # starting idx after special token\n",
        "        current_idx = len(self.word2idx)\n",
        "          # Add words to vocabulary\n",
        "        for word, _ in most_common:\n",
        "            self.word2idx[word] = current_idx\n",
        "            self.idx2word[current_idx] = word\n",
        "            current_idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text: str):\n",
        "        tokens = self.tokenize(text)\n",
        "        sequence = []\n",
        "\n",
        "        # Add start token\n",
        "        sequence.append(self.word2idx['<START>'])\n",
        "\n",
        "        # Converting each token to its corresponding index\n",
        "        # If token not in vocabulary, using UNK token\n",
        "        for token in tokens:\n",
        "            if token in self.word2idx:\n",
        "                sequence.append(self.word2idx[token])\n",
        "            else:\n",
        "                sequence.append(self.word2idx['<UNK>'])\n",
        "\n",
        "        # Add end token\n",
        "        sequence.append(self.word2idx['<END>'])\n",
        "        return sequence\n",
        "\n",
        "    def sequence_to_text(self, sequence: List[int]):\n",
        "        tokens = []\n",
        "        is_speaker = False\n",
        "        current_speaker = []\n",
        "\n",
        "        for idx in sequence:\n",
        "            token = self.idx2word[idx]\n",
        "\n",
        "            if token == '<SPEAKER>':\n",
        "                is_speaker = True\n",
        "                if current_speaker:  # Add previous speaker's text\n",
        "                    tokens.append(' '.join(current_speaker))\n",
        "                current_speaker = []\n",
        "            elif token == ':':\n",
        "                is_speaker = False\n",
        "                speaker_name = ' '.join(current_speaker)\n",
        "                tokens.append(f\"{speaker_name}:\")\n",
        "                current_speaker = []\n",
        "            elif token not in ['<START>', '<END>', '<PAD>', '<UNK>']:\n",
        "                if is_speaker:\n",
        "                    current_speaker.append(token)\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "\n",
        "        if current_speaker:  # Add any remaining speaker text\n",
        "            tokens.append(' '.join(current_speaker))\n",
        "\n",
        "        return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "tfc3rgD_rLmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing My Tokenizer"
      ],
      "metadata": {
        "id": "ajYnEYajEAuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample texts\n",
        "texts = [\n",
        "    \"hello world this is a test\",\n",
        "    \"another test with more words\",\n",
        "    \"hello again world testing tokenizer\",\n",
        "    \"this is the final test sentence\"\n",
        "]\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(max_vocab_size=20)\n",
        "\n",
        "# Build vocabulary\n",
        "tokenizer.build_vocab(texts)\n",
        "\n",
        "# Print vocabulary statistics\n",
        "print(f\"\\nVocabulary size: {len(tokenizer.word2idx)}\")\n",
        "print(\"\\nWord to index mapping:\")\n",
        "for word, idx in tokenizer.word2idx.items():\n",
        "    print(f\"{word}: {idx}\")\n",
        "\n",
        "# Test text to sequence conversion\n",
        "test_text = \"hello world test\"\n",
        "sequence = tokenizer.text_to_sequence(test_text)\n",
        "print(f\"\\nConverting '{test_text}' to sequence:\")\n",
        "print(f\"Sequence: {sequence}\")\n",
        "\n",
        "# Test sequence to text conversion\n",
        "recovered_text = tokenizer.sequence_to_text(sequence)\n",
        "print(f\"Recovered text: '{recovered_text}'\")\n",
        "\n",
        "# Test unknown word handling\n",
        "unknown_text = \"hello unknown word test\"\n",
        "unknown_sequence = tokenizer.text_to_sequence(unknown_text)\n",
        "print(f\"\\nTesting unknown word handling:\")\n",
        "print(f\"Input text: '{unknown_text}'\")\n",
        "print(f\"Sequence with unknown: {unknown_sequence}\")\n",
        "print(f\"Recovered text: '{tokenizer.sequence_to_text(unknown_sequence)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXRaZD1iyico",
        "outputId": "38e36ac2-74d5-41b5-bb77-f73f54e13c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size: 20\n",
            "\n",
            "Word to index mapping:\n",
            "<PAD>: 0\n",
            "<UNK>: 1\n",
            "<START>: 2\n",
            "<END>: 3\n",
            "<SPEAKER>: 4\n",
            ":: 5\n",
            "test: 6\n",
            "hello: 7\n",
            "world: 8\n",
            "this: 9\n",
            "is: 10\n",
            "a: 11\n",
            "another: 12\n",
            "with: 13\n",
            "more: 14\n",
            "words: 15\n",
            "again: 16\n",
            "testing: 17\n",
            "tokenizer: 18\n",
            "the: 19\n",
            "\n",
            "Converting 'hello world test' to sequence:\n",
            "Sequence: [2, 7, 8, 6, 3]\n",
            "Recovered text: 'hello world test'\n",
            "\n",
            "Testing unknown word handling:\n",
            "Input text: 'hello unknown word test'\n",
            "Sequence with unknown: [2, 7, 1, 1, 6, 3]\n",
            "Recovered text: 'hello test'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prepration"
      ],
      "metadata": {
        "id": "cy3UFaRV6kQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "IJ4mzrS-AxCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    # Convert html entities\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # Remove URLs but preserve speaker patterns\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove file attachments\n",
        "    text = re.sub(r'<file_\\w+>', '', text)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "DFDHlARJAylz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class"
      ],
      "metadata": {
        "id": "Ag3B8mPR8CeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SamsumDataset(Dataset):\n",
        "    def __init__(self, dialogues, summaries, tokenizer):\n",
        "\n",
        "        self.dialogues = []\n",
        "        for d in dialogues:\n",
        "            cleaned = clean_text(d)\n",
        "            self.dialogues.append(cleaned)\n",
        "\n",
        "\n",
        "        self.summaries = []\n",
        "        for s in summaries:\n",
        "            cleaned = clean_text(s)\n",
        "            self.summaries.append(cleaned)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dialogue = self.dialogues[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # Convert to sequences of indices\n",
        "        dialogue_seq = self.tokenizer.text_to_sequence(dialogue)\n",
        "        summary_seq = self.tokenizer.text_to_sequence(summary)\n",
        "\n",
        "        return {\n",
        "            'dialogue': torch.tensor(dialogue_seq),\n",
        "            'summary': torch.tensor(summary_seq)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Yltx9sl_8ES2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 512"
      ],
      "metadata": {
        "id": "N0VUIh0WT9va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch_with_padding(batch, max_seq_length=None):\n",
        "    # Separate dialogues and summaries\n",
        "    dialogues = []\n",
        "    summaries = []\n",
        "\n",
        "    # Ensure sequences don't exceed max_seq_length\n",
        "    for item in batch:\n",
        "        if max_seq_length:\n",
        "            dialogue = item['dialogue'][:max_seq_length]\n",
        "            summary = item['summary'][:max_seq_length]\n",
        "        else:\n",
        "            dialogue = item['dialogue']\n",
        "            summary = item['summary']\n",
        "        dialogues.append(dialogue)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Pad sequences\n",
        "    dialogues_padded = pad_sequence(dialogues, batch_first=True, padding_value=0)\n",
        "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'dialogues': dialogues_padded,\n",
        "        'summaries': summaries_padded\n",
        "    }\n"
      ],
      "metadata": {
        "id": "jmTgaJif8wDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(train_path, val_path, test_path, tokenizer, batch_size=32, max_seq_length=512):\n",
        "    \"\"\"\n",
        "    Load and prepare dialogue data for training\n",
        "    \"\"\"\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    # Clean and convert data to strings\n",
        "    for df in [train_df, val_df, test_df]:\n",
        "        df['dialogue'] = df['dialogue'].fillna('').astype(str)\n",
        "        df['summary'] = df['summary'].fillna('').astype(str)\n",
        "\n",
        "    # Build vocabulary from cleaned training data\n",
        "    print(\"Building vocabulary...\")\n",
        "    tokenizer.build_vocab(\n",
        "        list(train_df['dialogue']) + list(train_df['summary'])\n",
        "    )\n",
        "    print(f\"Vocabulary size: {len(tokenizer.word2idx)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SamsumDataset(\n",
        "        train_df['dialogue'].tolist(),\n",
        "        train_df['summary'].tolist(),\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    val_dataset = SamsumDataset(\n",
        "        val_df['dialogue'].tolist(),\n",
        "        val_df['summary'].tolist(),\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    test_dataset = SamsumDataset(\n",
        "        test_df['dialogue'].tolist(),\n",
        "        test_df['summary'].tolist(),\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    # Create dataloaders with max sequence length\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: prepare_batch_with_padding(x, max_seq_length)\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: prepare_batch_with_padding(x, max_seq_length)\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: prepare_batch_with_padding(x, max_seq_length)\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "ST01KX_R9xSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Data prepration Pipeline"
      ],
      "metadata": {
        "id": "fZlzthAPD-gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add data loading and preparation\n",
        "train_path = \"/content/samsum-train.csv\"\n",
        "val_path = \"/content/samsum-validation.csv\"\n",
        "test_path = \"/content/samsum-test.csv\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(max_vocab_size=30000)\n",
        "\n",
        "# Load and prepare data\n",
        "train_loader, val_loader, test_loader = load_and_prepare_data(\n",
        "    train_path,\n",
        "    val_path,\n",
        "    test_path,\n",
        "    tokenizer,\n",
        "    batch_size=32\n",
        "\n",
        ")\n",
        "\n",
        "# Print sample batch\n",
        "# for batch in train_loader:\n",
        "#     print(\"\\nSample batch shapes:\")\n",
        "#     print(f\"Dialogues: {batch['dialogues'].shape}\")\n",
        "#     print(f\"Summaries: {batch['summaries'].shape}\")\n",
        "\n",
        "#     # Print first dialogue and summary from batch\n",
        "#     print(\"\\nSample dialogue and summary:\")\n",
        "#     dialogue_seq = batch['dialogues'][0].tolist()  # Get first dialogue\n",
        "#     summary_seq = batch['summaries'][0].tolist()  # Get first summary\n",
        "\n",
        "#     print(\"\\nDialogue:\")\n",
        "#     print(tokenizer.sequence_to_text(dialogue_seq))\n",
        "#     print(\"\\nSummary:\")\n",
        "#     print(tokenizer.sequence_to_text(summary_seq))\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3j_4TwO-KRq",
        "outputId": "b9d22acb-7d77-456a-8180-d5b2e86a18ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabulary...\n",
            "Vocabulary size: 29998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Pipeline"
      ],
      "metadata": {
        "id": "Iymn1vBGt-vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Masks"
      ],
      "metadata": {
        "id": "weVTWCXSwpk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(src, tgt):\n",
        "    # Source mask (for padding)\n",
        "    src_padding_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # Target mask for padding\n",
        "    tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # Target mask for causal attention\n",
        "    seq_length = tgt.size(1)\n",
        "    causal_mask = torch.triu(torch.ones((seq_length, seq_length)) * float('-inf'), diagonal=1)\n",
        "    causal_mask = causal_mask.to(tgt.device)\n",
        "\n",
        "    # Combine padding and causal masks for target\n",
        "    tgt_mask = causal_mask.masked_fill(tgt_padding_mask == 0, float('-inf'))\n",
        "\n",
        "    return src_padding_mask, tgt_mask"
      ],
      "metadata": {
        "id": "z6Ku6LbZuJk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Functions"
      ],
      "metadata": {
        "id": "uwfpjtguxD_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(output, target, pad_idx=0):\n",
        "    # Get predictions\n",
        "    _, predictions = output.max(1)\n",
        "\n",
        "    # Create mask to ignore padding tokens\n",
        "    mask = (target != pad_idx)\n",
        "\n",
        "    # Calculate accuracy only on non-padded tokens\n",
        "    correct = ((predictions == target) * mask).sum().float()\n",
        "    total = mask.sum().float()\n",
        "\n",
        "    # Avoid division by zero\n",
        "    accuracy = correct / total if total > 0 else torch.tensor(0.0)\n",
        "\n",
        "    return accuracy.item()"
      ],
      "metadata": {
        "id": "Dx4R82np6A3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, device, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f'Training')\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        src = batch['dialogues'].to(device)\n",
        "        tgt = batch['summaries'].to(device)\n",
        "\n",
        "        # Create target input (remove last token) and target output (remove first token)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        # Create masks\n",
        "        src_mask, tgt_mask = create_masks(src, tgt_input)\n",
        "        src_mask = src_mask.to(device)\n",
        "        tgt_mask = tgt_mask.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        # Reshape output and target for loss calculation\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        # Calculate accuracy with padding handling\n",
        "        acc = calculate_accuracy(output, tgt_output, pad_idx=0)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Step the scheduler after each batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "\n",
        "        # Update progress bar with current learning rate\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        avg_acc = total_acc / (batch_idx + 1)\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        progress_bar.set_description(\n",
        "            f'Training: loss={avg_loss:.4f}, acc={avg_acc:.4f}, lr={current_lr:.6f}'\n",
        "        )\n",
        "\n",
        "    return total_loss / num_batches, total_acc / num_batches"
      ],
      "metadata": {
        "id": "YJHGOgOVxJZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    # Create progress bar with loss and accuracy tracking\n",
        "    progress_bar = tqdm(val_loader, desc=f'Validating')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            src = batch['dialogues'].to(device)\n",
        "            tgt = batch['summaries'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
        "            src_mask = src_mask.to(device)\n",
        "            tgt_mask = tgt_mask.to(device)\n",
        "\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, tgt_output)\n",
        "\n",
        "            # Calculate accuracy with padding handling\n",
        "            acc = calculate_accuracy(output, tgt_output, pad_idx=0)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "\n",
        "            # Update progress bar description with current loss and accuracy\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            avg_acc = total_acc / (batch_idx + 1)\n",
        "            progress_bar.set_description(f'Validating: loss={avg_loss:.4f}, acc={avg_acc:.4f}')\n",
        "\n",
        "    return total_loss / num_batches, total_acc / num_batches"
      ],
      "metadata": {
        "id": "dzZRNzxuzq_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "3rI5qnwxztkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "SRC_VOCAB_SIZE = 30000\n",
        "TGT_VOCAB_SIZE = 30000\n",
        "DIM_MODEL = 256\n",
        "NUM_HEADS = 4\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "HIDDEN_DIM = 1024\n",
        "DROPOUT = 0.1\n",
        "MAX_LR = 0.01  # Maximum learning rate for OneCycleLR\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "ks8FEJXJzvR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = Transformer(encoder_vocab_size=SRC_VOCAB_SIZE,decoder_vocab_size=TGT_VOCAB_SIZE,dim_model=DIM_MODEL,num_heads=NUM_HEADS,num_encoder_layers=NUM_ENCODER_LAYERS,num_decoder_layers=NUM_DECODER_LAYERS,hidden_dim=HIDDEN_DIM,dropout=DROPOUT,max_seq_length=MAX_SEQ_LENGTH).to(device)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=MAX_LR)\n",
        "\n",
        "# Calculate total steps for OneCycleLR\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "\n",
        "# Initialize the OneCycleLR scheduler\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=MAX_LR,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.3,\n",
        "    div_factor=25,\n",
        "    final_div_factor=1e4,\n",
        "    anneal_strategy='cos'  # Cosine annealing\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "pvpivJ-nz4Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{NUM_EPOCHS}')\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, scheduler)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS} - '\n",
        "          f'train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, '\n",
        "          f'val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}, '\n",
        "          f'lr: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, 'best_transformer_model.pt')\n",
        "        print('Model saved!')\n"
      ],
      "metadata": {
        "id": "Z0OQvLFF0OcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(model, tokenizer, dialogue, device, max_length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            # Tokenize and prepare input\n",
        "            dialogue_seq = tokenizer.text_to_sequence(dialogue)\n",
        "            src = torch.tensor([dialogue_seq], device=device)\n",
        "\n",
        "            # Create source mask\n",
        "            src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Generate sequence\n",
        "            generated_seq = model.generate(\n",
        "                src,\n",
        "                src_mask,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            return tokenizer.sequence_to_text(generated_seq.squeeze().tolist())\n",
        "        except Exception as e:\n",
        "            print(f\"Error in generate_summary: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "def test_model_generation():\n",
        "    # Load the trained model\n",
        "\n",
        "      checkpoint = torch.load('best_transformer_model.pt', map_location=device)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      model.eval()\n",
        "\n",
        "      # Test dialogue\n",
        "      test_dialogue = \"John: Hi Mary, how are you? Mary: I'm good, thanks! How about you? John: I'm doing great, thanks for asking!\"\n",
        "\n",
        "      # Generate summary with different temperatures\n",
        "      for temp in [0.7, 1.0, 1.3]:\n",
        "          summary = generate_summary(model, tokenizer, test_dialogue, device, temperature=temp)\n",
        "          print(f\"\\nTemperature: {temp}\")\n",
        "          print(f\"Dialogue:\\n{test_dialogue}\\n\")\n",
        "          print(f\"Generated Summary:\\n{summary}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ykf0PudJCElb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}